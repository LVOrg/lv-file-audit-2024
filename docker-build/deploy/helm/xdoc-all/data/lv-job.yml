# Thou! please carefully reading to avoid mistake !!!!!
# This Helm values declare all you need to deploy LV-File-Service to Developer environment, On Premise infrastructure or AWS

# All pods in this deployment require below info (regardless where they would be installed):
#  Tất cả các Pod trong quá trình triển khai này đều yêu cầu thông tin bên dưới (bất kể chúng sẽ được cài đặt ở đâu):

# 1 - MongodDb database, embody by tuples of :  host, authSource, username, password. My Lambkin! do not worry about each infor that. The explanation will be show each line in this Helm Values.
# 2- Need one database in MongoDb to manage all Tenants even if File-Service just sevrve for only one Tenant. (Cần một cơ sở dữ liệu trong MongoDb để quản lý tất cả các Đối tượng thuê ngay cả khi Dịch vụ tệp chỉ phục vụ cho một Đối tượng thuê.)
# 3- At Least one ElasticSearch in which embody in tuples of: server and prefix_index ('prefix_index will be explained when it was declare')
# 4- One broker message. When Pod wanna to inform to others. The Pod could not directly  inform. It just informs to broker message and broker message will inform to another.
# 5- One temporary storage
#    data
#    ├── aws.yml                             # AWS deployment-value
#    ├── lacviet.yml                         # Lac Viet deployment-value
#    ├── dev.yml  (*****)                    # Dev and QC deployment-value, this is also a sample for all deployments value suchsa : 'lacviet' or 'aws'
#    ├── jobs.app.yml                        # List of File-Job values
#    └── job.cron.yml                        # List of Cron-Job values, all Cron Job POD run one time
#    templates
#    ├── web.apps.yml                        # Web API pod deployment
#    ├── web.config.yml                      # All config for Web API
#    ├── web.namespace.yml                   # Namespace for Web API, wrong namespace will make chain of error,  through and through all PODs
#    ├── web.service.yml                     # Make a service link to web app. The service allow another POD can access to Web Api app
#    ├── jobs.config.yml                     # All configuration infor serve for JOB and cron job
#    ├── jobs.apps.yml                       # All apps in which File-Processing run
#    ├── jobs.cron.yml                       # All dataset deployment
#    ├── jobs.namespace.yml                  # Namespace for Job and cron job deploy
#    ├── long.horn.namespace.yml             # Longhorn namespace.Till now, Longhorn is unofficially volume (pilot test)
#    └── long.horn.yml                       # Longhorn claim
#
#apiVersion: v1


config:
  db:
    cnn__: mongodb://admin:Lacviet#123@172.16.0.85:27017/admin
    cnn: mongodb%3A//172.16.0.83%3A27017%2C172.16.0.84%3A27017%2C172.16.0.85%3A27017/%3FreplicaSet%3Drs0%26readPreference%3Dsecondary
  # The clarefication of MongoDB or Mongos Routers
    admin_db_name: 'lv-docs'
    # File-Service need one database in a certain Mongodb to manage all Tenants.
    # 'admin_db_name' is a crucial database.
    # Wrong 'admin_db_value' will cause chain of crash.
    # When File-Service or File-API start,
    # it will detect if 'admin-db-name' in a certain Mongodb is existing.
    # if 'admin_db_name' was not found File-Service or File-API will automatically create new database
    # name by value of 'admin_db_name' in a certain MongoDB. So,
    #wrong value of admin_db_name that mean new system was created.
    #This is required value even if File-Server or File-API just serve for only one Tenant.

    # The rule of password
  elastic_search:
  # clarification of ElasticSearch
    prefix_index: 'qtsc-codx'
    # File-Service and File-API need a certain ElasticSearch
    # where readable content of File Upload was index in.
    # Index-ElasticSearch and MongoDB Tenant Database is corresponding to Managed-Tenants.
    # For ElasticSearch do not have something call Database.
    # ElasticSearch use Index-Name lieu of Database.
    # When API-File and File-Service create new Tenant they also create new ElasticSearch
    # Index by concat 'prefix_index' and Tenant-Name.
    # Example Tenant-Name is qc, ElasticSearch Index Name is lv-codx_qc
    server: 'http://172.16.0.83:9200'
    # In poor conditional, we have only one ElasticServer Node
    # if thee's system need more than one node this value will be a list of url separated by comma

#  rabbitmq:
#    port: 31674
#    server: 172.16.7.91
#    msg: codx-lv
  rabbitmq:
  # Broker message. All PODs will use Broker-Message to communicating togethers
    port: '5672'
    #Port
    server: 'rabbitmq.rabbitmq-dev.svc.cluster.local'
    #Broker server

    msg: file-codx
#storage:
#  # This configuration  describes how and where all Pods in all deployments use shared-files
#  # when Pods need to process of file or,
#  # when the PODs need create, update or delete file, the PODs will use this location to do that
#  server: 10.0.2.217
#  # Server where share directory locate
#  directory: /var/nfs_share_dir_for_k8s/xdoc-web
#  # The full path to share-directory at storage server
#  # Read below carefully before use share directory for K8S
#  # 1- The share directory is belonged no-one use chown comand to do that.
#  # Example: chown nodbody:nobody  /var/nfs_share_dir_for_k8s so all PODs in other worker can access.
#  # Thou may check by "stat /var/nfs_share_dir_for_k8s"
#  #
#  name: xdoc-storage


#172.16.0.85/data/share-data/lv-file
storage:
  # This configuration  describes how and where all Pods in all deployments use shared-files
  # when Pods need to process of file or,
  # when the PODs need create, update or delete file, the PODs will use this location to do that
  server: 172.16.0.85
  # Server where share directory locate
  directory: /data/share-data/lv-file/tmp
  # The full path to share-directory at storage server
  # Read below carefully before use share directory for K8S
  # 1- The share directory is belonged no-one use chown comand to do that.
  # Example: chown nodbody:nobody  /var/nfs_share_dir_for_k8s so all PODs in other worker can access.
  # Thou may check by "stat /var/nfs_share_dir_for_k8s"
  # mount -t nfs 192.168.18.36:/codx-file-storage/files /from-192-168-18-36
  name: xdoc-storage

storage_files:
  server: 172.16.0.85
  directory: /data/share-data/lv-file/files
  name: storage-files
#volumes:
#        - name: xdoc-temp-storage
#          nfs:
#            path: /data/share/xdoc-web
#            server: 172.16.0.84
storage__:
  # This configuration  describes how and where all Pods in all deployments use shared-files
  # when Pods need to process of file or,
  # when the PODs need create, update or delete file, the PODs will use this location to do that
  server: 172.16.0.84
  # Server where share directory locate
  directory: /data/share/xdoc-web
  # The full path to share-directory at storage server
  # Read below carefully before use share directory for K8S
  # 1- The share directory is belonged no-one use chown comand to do that.
  # Example: chown nodbody:nobody  /var/nfs_share_dir_for_k8s so all PODs in other worker can access.
  # Thou may check by "stat /var/nfs_share_dir_for_k8s"
  #
  name: xdoc-storage

fileJob:
  namespace: xdoc-job-v1
  configMapName: xdoc-job-config
  replicas: 2
  repository: nttlong/py310-xdoc
  pullPolicy: IfNotPresent
  tag: amd.cpu.9.39.44
  mountPath: /app/share-storage
  vn_suggest_url: vn-suggestion.xdoc-web:8000
  tika_server: http://tika.xdoc:9998
  web_api_url: https://codx.lacviet.vn/lvfile
