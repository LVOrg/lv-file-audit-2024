# Thou! please carefully reading to avoid mistake !!!!!
# This Helm values declare all you need to deploy LV-File-Service to Developer environment, On Premise infrastructure or AWS

# All pods in this deployment require below info (regardless where they would be installed):
#  Tất cả các Pod trong quá trình triển khai này đều yêu cầu thông tin bên dưới (bất kể chúng sẽ được cài đặt ở đâu):

# 1 - MongodDb database, embody by tuples of :  host, authSource, username, password. My Lambkin! do not worry about each infor that. The explanation will be show each line in this Helm Values.
# 2- Need one database in MongoDb to manage all Tenants even if File-Service just sevrve for only one Tenant. (Cần một cơ sở dữ liệu trong MongoDb để quản lý tất cả các Đối tượng thuê ngay cả khi Dịch vụ tệp chỉ phục vụ cho một Đối tượng thuê.)
# 3- At Least one ElasticSearch in which embody in tuples of: server and prefix_index ('prefix_index will be explained when it was declare')
# 4- One broker message. When Pod wanna to inform to others. The Pod could not directly  inform. It just informs to broker message and broker message will inform to another.
# 5- One temporary storage
#    data
#    ├── aws.yml                             # AWS deployment-value
#    ├── lacviet.yml                         # Lac Viet deployment-value
#    ├── dev.yml  (*****)                    # Dev and QC deployment-value, this is also a sample for all deployments value suchsa : 'lacviet' or 'aws'
#    ├── jobs.app.yml                        # List of File-Job values
#    └── job.cron.yml                        # List of Cron-Job values, all Cron Job POD run one time
#    templates
#    ├── web.apps.yml                        # Web API pod deployment
#    ├── web.config.yml                      # All config for Web API
#    ├── web.namespace.yml                   # Namespace for Web API, wrong namespace will make chain of error,  through and through all PODs
#    ├── web.service.yml                     # Make a service link to web app. The service allow another POD can access to Web Api app
#    ├── jobs.config.yml                     # All configuration infor serve for JOB and cron job
#    ├── jobs.apps.yml                       # All apps in which File-Processing run
#    ├── jobs.cron.yml                       # All dataset deployment
#    ├── jobs.namespace.yml                  # Namespace for Job and cron job deploy
#    ├── long.horn.namespace.yml             # Longhorn namespace.Till now, Longhorn is unofficially volume (pilot test)
#    └── long.horn.yml                       # Longhorn claim
config:
  auto_ssl_redirect: off
  db:
    cnn: mongodb%3A//172.16.0.83%3A27017%2C172.16.0.84%3A27017%2C172.16.0.85%3A27017/%3FreplicaSet%3Drs0%26readPreference%3Dsecondary
  # The clarefication of MongoDB or Mongos Routers
    admin_db_name: 'lv-docs'

  elastic_search:
  # clarification of ElasticSearch
    prefix_index: 'qtsc-codx'
    # File-Service and File-API need a certain ElasticSearch
    # where readable content of File Upload was index in.
    # Index-ElasticSearch and MongoDB Tenant Database is corresponding to Managed-Tenants.
    # For ElasticSearch do not have something call Database.
    # ElasticSearch use Index-Name lieu of Database.
    # When API-File and File-Service create new Tenant they also create new ElasticSearch
    # Index by concat 'prefix_index' and Tenant-Name.
    # Example Tenant-Name is qc, ElasticSearch Index Name is lv-codx_qc
    server: 'http://172.16.0.83:9200'
    # In poor conditional, we have only one ElasticServer Node
    # if thee's system need more than one node this value will be a list of url separated by comma
  rabbitmq___:
    port: 31674
    server: 172.16.7.91
    msg: codx-lv
  rabbitmq:
  # Broker message. All PODs will use Broker-Message to communicating togethers
    port: '5672'
    #Port
    server: 'rabbitmq.rabbitmq-dev.svc.cluster.local'
    #Broker server

    msg: file-codx

storage___:
  # This configuration  describes how and where all Pods in all deployments use shared-files
  # when Pods need to process of file or,
  # when the PODs need create, update or delete file, the PODs will use this location to do that
  server: 172.16.0.84
  # Server where share directory locate
  directory: /data/share/xdoc-web
  # The full path to share-directory at storage server
  # Read below carefully before use share directory for K8S
  # 1- The share directory is belonged no-one use chown comand to do that.
  # Example: chown nodbody:nobody  /var/nfs_share_dir_for_k8s so all PODs in other worker can access.
  # Thou may check by "stat /var/nfs_share_dir_for_k8s"
  #
  name: xdoc-storage
#172.16.0.85/data/share-data/lv-file
storage:
  # This configuration  describes how and where all Pods in all deployments use shared-files
  # when Pods need to process of file or,
  # when the PODs need create, update or delete file, the PODs will use this location to do that
  server: 172.16.0.85
  # Server where share directory locate
  directory: /data/share-data/lv-file/tmp
  # The full path to share-directory at storage server
  # Read below carefully before use share directory for K8S
  # 1- The share directory is belonged no-one use chown comand to do that.
  # Example: chown nodbody:nobody  /var/nfs_share_dir_for_k8s so all PODs in other worker can access.
  # Thou may check by "stat /var/nfs_share_dir_for_k8s"
  # mount -t nfs 192.168.18.36:/codx-file-storage/files /from-192-168-18-36
  name: xdoc-storage

storage_files:
  server: 172.16.0.85
  directory: /data/share-data/lv-file/files
  name: storage-files
storage_longhorn:
  # If thou would like to use longhorn-storage change storage_longhorn into storage.
  size: 30Gi
  # storage request size is at least 30 GB (why? some Pods in deployment need AI-dataset.
  # The volume for AI-dataset, sometime reach 30GB. So, 30GB is OK )
  name: xdoc-storage
  className: longhorn-storage-delete
  namespace: xdoc-storage

webApi:
# This is the value of web api configuration
  name: xdoc
  #name of web api service in K8S
  # This value really extremely important.
  # If wrong value no-one can access Web Api. As usual,
  # this is unchangeable value. But, when thou deploy in a certain K8S with ingress-controller.
  # Thou must inquire thee's DEV-OP about ingress-controller and get correct name
  namespace: xdoc
  # namespace: The same rule of name
  hostUrl: https://codx.lacviet.vn/lvfile
  # inquire about Web Host server from thee's DEV-OP
  configMapName: xdoc-config
  replicas: 8
  # number of replicate, depend on numof K8S workers
  repository:  nttlong/web-api
  tag: amd.1
  #The tag name is describe how many stage in image:
  # rc: release candidate
  # 1 - The first number is embodied of the generation of image, it is also a first stage of image
  # where all necessary components at OS level support for all PODs run  (till now is the first generation)
  # 2 - The second number is embodied of the Open-Core-Stage where Open-Core-Stage were installed.
  # Those libraries did not write at Lac Viet. Such as MongoDb driver, pytorch,...
  # 3 - The third number is embodied of the Core-Stage .
  #     All core-library were write and compile at Lac Viet.
  # 4 - The final number is embodied of End-Point Pod start .
  #     All End-Point Pod were write and compile at Lac Viet.
  mountPath: /app/share-storage
fileJobUnused:
  namespace: xdoc-job-v7
  configMapName: xdoc-job-config
  replicas: 2
  repository: nttlong/py310-xdoc
  pullPolicy: IfNotPresent
  tag: amd.cpu.9.31.17
  mountPath: /app/share-storage